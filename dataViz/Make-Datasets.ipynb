{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a messy file that was used iteratively to make [Extended-Full]LatLong-tweet-test.csv and all of the day csv's. Refer to the comments to see what each part really does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point,Polygon\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_columbia = gpd.read_file('../municipal_boundaries/MGN2012_MUNICIPIO_URBANCLUSTERS.shp')\n",
    "df_tweets = pd.read_csv('../data/fake_tweets.csv')\n",
    "df_centroids = pd.read_csv('./region-to-centroid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extracts the region (index of the shapefile) from a latitude and longitude\n",
    "def long_lat_to_region(df, long, lat):\n",
    "    bp = Point(long, lat)\n",
    "    for index, row in df.iterrows():\n",
    "        if bp.within(row['geometry']):\n",
    "            return index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a hashmap that remembers a region's name to its index. It is used as to memoize the lookup process.\n",
    "location_to_index = None\n",
    "with open('region_to_region_index.json', 'r') as fp:\n",
    "    location_to_index = json.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this takes the fake tweets and converts the latitude and longitude's into region numbers and each region's centroid\n",
    "# this will make a comprehensive dataset called [Extended-Full]LatLong-tweet-test.csv that will be used during visualization\n",
    "toAddOldRegion = []\n",
    "toAddNewRegion = []\n",
    "\n",
    "toAddOldLong = []\n",
    "toAddOldLat = []\n",
    "\n",
    "toAddNewLong = []\n",
    "toAddNewLat = []\n",
    "\n",
    "for i in range(len(df_tweets)):\n",
    "    if (i % 1000 == 0):\n",
    "        print(i)\n",
    "    data = df_tweets.iloc[i]\n",
    "\n",
    "    old_region = long_lat_to_region(df_columbia, data.Longitude, data.Latitude)\n",
    "    new_region = long_lat_to_region(df_columbia, data.New_Long, data.New_Lat)\n",
    "    \n",
    "    old_centroid = df_centroids[df_centroids['Name'] == old_region]\n",
    "    new_centroid = df_centroids[df_centroids['Name'] == new_region]\n",
    "    \n",
    "    if (len(old_centroid) == 0 or len(new_centroid) == 0):\n",
    "        toAddOldRegion.append(None)\n",
    "        continuefr\n",
    "    \n",
    "    location_to_index[data.Location] = old_region\n",
    "    location_to_index[data.New_Loc] = new_region\n",
    "    \n",
    "    toAddOldRegion.append(old_region)\n",
    "    toAddNewRegion.append(new_region)\n",
    "    \n",
    "    toAddOldLong.append(old_centroid.Longitude.values[0])\n",
    "    toAddOldLat.append(old_centroid.Latitude.values[0])\n",
    "    \n",
    "    toAddNewLong.append(new_centroid.Longitude.values[0])\n",
    "    toAddNewLat.append(new_centroid.Latitude.values[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets['Old Region'] = toAddOldRegion\n",
    "# drops rows where match wasn't found\n",
    "df_tweets.dropna(subset=['Old Region'], inplace=True)\n",
    "df_tweets['New Region'] = toAddNewRegion\n",
    "\n",
    "df_tweets['Old_Region_Long'] = toAddOldLong\n",
    "df_tweets['Old_Region_Lat'] = toAddOldLat\n",
    "\n",
    "df_tweets['New_Region_Long'] = toAddNewLong\n",
    "df_tweets['New_Region_Lat'] = toAddNewLat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the new csv\n",
    "\\df_tweets.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gets how many people went from one region to another\n",
    "df_dist = df_tweets.groupby(['Old Region', 'New Region']).count().reset_index().nlargest(20, 'UserID')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplifies the dataframe to show only what's important\n",
    "df_clean = pd.DataFrame.from_dict({'Old Region': df_dist['Old Region'], 'New Region': df_dist['New Region'], 'Number': df_dist['UserID']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this creates a csv of the top 20 regional movements\n",
    "df_clean.to_csv('../data/movement.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_dist_full = df_tweets.groupby(['Old Region', 'New Region']).count().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clean_full = pd.DataFrame.from_dict({'Old Region': df_dist_full['Old Region'], 'New Region': df_dist_full['New Region'], 'Number': df_dist_full['UserID']})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this creates a csv of all regional movements\n",
    "df_clean_full.to_csv('../data/movement_full.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this was done AFTER. this reads the csv's created. Note they were moved by me to the same directory, although the code saved them elsewhere.\n",
    "df_move = pd.read_csv('movement.csv')\n",
    "df_loc = pd.read_csv('region-to-centroid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uses df_loc to turn a region number into the centroid\n",
    "def apply_long_find(region):\n",
    "    return df_loc[df_loc['Name'] == region].Longitude.values[0]\n",
    "\n",
    "def apply_lat_find(region):\n",
    "    return df_loc[df_loc['Name'] == region].Latitude.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_move['Old Longitude'] = df_move['Old Region'].apply(apply_long_find)\n",
    "df_move['Old Latitude'] = df_move['Old Region'].apply(apply_lat_find)\n",
    "df_move['New Longitude'] = df_move['New Region'].apply(apply_long_find)\n",
    "df_move['New Latitude'] = df_move['New Region'].apply(apply_lat_find)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# refines movement.csv to be the centroid's of each of the regions\n",
    "df_move.to_csv('movement.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/LatLong-tweet-test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.dropna(subset=['created_at'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# created the Year, Month, Day, Hour, Minute, and Second columns of [Extended-Full]LatLong-tweet-test.csv and the day csvs.\n",
    "def parse_date_col(df, datecol):\n",
    "    years = []\n",
    "    months = []\n",
    "    days = []\n",
    "    hours = []\n",
    "    minutes = []\n",
    "    seconds = []\n",
    "    for index, row in df.iterrows():\n",
    "        large, small = row[datecol].split(' ')\n",
    "        year, month, day = large.split('-')\n",
    "        hour, minute, second = small.split(':')\n",
    "        years.append(year)\n",
    "        months.append(month)\n",
    "        days.append(day)\n",
    "        hours.append(hour)\n",
    "        minutes.append(minute)\n",
    "        seconds.append(second)\n",
    "    df['Year'] = years\n",
    "    df['Month'] = months\n",
    "    df['Day'] = days\n",
    "    df['Hour'] = hours\n",
    "    df['Minute'] = minutes\n",
    "    df['Second'] = seconds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parse_date_col(df_tweets, 'created_at')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tweets.to_csv('../data/[Extended-Full]LatLong-tweet-test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reopens the hashmap for further use\n",
    "location_to_index = None\n",
    "with open('region_to_region_index.json', 'r') as fp:\n",
    "    location_to_index = json.load(fp)\n",
    "  \n",
    "# function that checks the hashmap and if it's not in the hashmap, it updates the hashmap (memoization) after doing the expensive lookup process\n",
    "def region_to_region_index(region, long, lat):\n",
    "    if region not in location_to_index:\n",
    "        location_to_index[region] = long_lat_to_region(df_columbia, long, lat)\n",
    "    return location_to_index[region]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/[Extended]LatLong-tweet-test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(subset=['Longitude', 'Latitude'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "toAdd = []\n",
    "for index, row in df.iterrows():\n",
    "    try:\n",
    "        toAdd.append(region_to_region_index(row['Location'], float(row['Longitude']), float(row['Latitude'])))\n",
    "    except:\n",
    "        toAdd.append(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Region_Number'] = toAdd\n",
    "df.dropna(subset=['Region_Number'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creates the day csvs!\n",
    "for name, group in df_tweets.groupby(['Day']):\n",
    "    file = '[Day ' + str(name) + ']-tweets.csv'\n",
    "    group.sort_values(by=['Hour'], inplace=True)\n",
    "    group.to_csv('./' + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saves hashmap\n",
    "with open('region_to_region_index.json', 'w') as fp:\n",
    "    json.dump(location_to_index, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
